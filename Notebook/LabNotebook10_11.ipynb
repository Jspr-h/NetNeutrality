{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool tracks\n",
    "\n",
    "### ArcGIS\n",
    "In the tooltrack arcGIS, we work with geographic information system (GIS) that allows us to work with maps and geographical information. The relevance of this tooltrack for our is not great but it is none the less very interesting. ArcGIS could provide us with information of our tweeters (both bots and humans) location in the world along with information of that specific location. Unfortunately, a lot (almost all) of the tweets have no location attached or the data has to be cleaned and looked through in order to establish if some location information is presented in the wrong context. We have tried to present our data set from TCAT in ArcGIS but it only allows us to see the location of about 23 tweets. We have created a web app, that provides information of where in the world, there is activity in relation to the hashtags used. See the web app [here](https://arcg.is/1O9bH9). NB!: Data has not been cleaned, all tweets are therefore not relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scientometrics and semantic\n",
    "\n",
    "By the use of scientometric analysis using scopus found 20 academic papers in the topic social bots. The analysis was based on the key words written by the authors.\n",
    "Then we have then carried out a semantic analysis on the abstracts of the 20 academic papers – hence here we did not rely on the authors key words but on a text analysis of the full abstracts. The analysis ended out in an illustration showing two clusters of articles – We are still to interrogate the differences of the two clusters – however our gut fell is that one side discusses bots from a technical view whereas the other cluster discussed bots based on a humanistic approach. \n",
    "Next steps – we will try to use our exam project data (from Twitter and FCC) and run a semantic analysis on those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau \n",
    "\n",
    "In the tooltrack on Tableau, we have learned to import data from various sources. For our project, it will be valuable to look at the different hashtags in our dataset from TCAT to get an overview of the different hashtags and users. Further, Tableau is an interesting tool when looking at our definitions of bots especially in relation to 'user profile feature', ' temporal behaviour features' as well as the different semantics and syntaxes. The tool track provides us with a more in depth approach to the programme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook \n",
    "\n",
    "The tool track in Facebook has proven not to have a great relevancy for our project as we mainly focus on twitter data and data from the FCC website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions of Bots\n",
    "\n",
    "We looked at different definitions of bots to narrow down our scope, although a fixed definition of bots does not exist yet, these are meant as guidelines for detecting bots. \n",
    "\n",
    "So far, we have two main definitons; broadband for America and Botometer (Indiana State University). They both provide us with different definitions of bots. Our current goal is two use these defintions on our data sets to pinpoint the differences among them highlighting what could potentially be classified as a bot. This will hopefully result in our own and more accurate definition of a bot. \n",
    "\n",
    "Below we have exemplified the definitions: \n",
    "\n",
    "Bots are not a new phenomenon, but the current usage of social, online bots have risen, although no one really knows the full scope of the problem. This is a result of the nature of the bots; to function like an integrated part of the system. A sphere that has indeed seen the rise of the robots, is the political sphere. Different measurements and criterias have been put up to exclude bots, when analysing content, but as bots evolve, it just keeps getting harder to distinguish who really sits behind the keyboard. \n",
    "\n",
    "Typically the parameters set up involves some data cleaning. Generally, this involves looking at user features, friends, timing, content and sentiment. This will be described more thoroughly through the text.  \n",
    "\n",
    "These are some of the different definitions that have been set up to highlight bots. \n",
    "Broadband for America\n",
    "Background: Emprata published a paper on the submitted comments to the FCC (Federal Communications Commission), analysing whether the comment sentiment was in favor/against the repeal of Title II. By setting up different parameters under which to analyse, it was found that more than 80% of comments were made by bots. Note, that this isn’t directed at highlighting bots on twitter, but on the website of the FCC. \n",
    "\n",
    "This is a review of the parameters they set up to distinguish bots from real users. It doesn’t eliminate bots from the dataset, but gives a possible measurement of how many of the comments made by bots. Instead they analysed the comments into different groupings of aforementioned parameters.\n",
    "\n",
    "#### Entire Dataset\n",
    "ALL - First they looked at all the comments made. \n",
    "Consider only domestic comments - Then they narrowed it down, only looking at domestic comments\n",
    "Consider only international comments - Only looking at international comments\n",
    "EXCLUDE FAKEMAILGENERATOR - excluding comments made by temporary e-mail address services. (einrot.com, jourrapide.com, armyspy.com, fleckens.hu, cuvox.de, rhyta.com, gustr.com, superrito.com, teleworm.com)\n",
    "\n",
    "Unique comments\n",
    "UNIQUE COMMENTS – Number of unique comments in the docket\n",
    "TRULY UNIQUE COMMENTS – Number of comments appearing only once in the docket\n",
    "\n",
    "#### Eliminating Duplicative Comments \n",
    " • ONE PER ADDRESS – Considers only the first comment for each unique address, city, state, ZIP code combination\n",
    " • ONE PER EMAIL – Considers only the first comment from each email address \n",
    "• ONE PER ADDRESS/EMAIL – Considers only the first comment from each unique address and email combination \n",
    "\n",
    "Considering only Valid Addresses (Considers sample address data through August 4, 2017) \n",
    "• VALID ADDRESS – Considers only comments where an exact address match was found     \n",
    "• ONE PER VALID ADDRESS - Considers only the first comment from each valid address \n",
    "\n",
    "To prepare the     data for analysis, we    performed various data cleanup/transformation    activities, including the following: \n",
    "• Removal of special characters, tabs, new lines, leading spaces, and trailing spaces\n",
    "• Transformation of text fields    to upper case\n",
    " • Standardization of state field to two character State Abbreviation\n",
    " • Standardization of ZIP code field to five digits, including padding    of leading zeros\n",
    " • Extraction of     country name    from the “international    address” field • Removal of duplicate    comment (allowing only one    per submission ID)\n",
    "\n",
    "#### DARPA Twitter bot challenge \n",
    "(BotOrNot competed in this challenge and came in top 3)\n",
    "https://arxiv.org/ftp/arxiv/papers/1601/1601.05140.pdf - Bot detection approaches\n",
    "\n",
    "#### Background: \n",
    "In 2015, DARPA (Defense advanced research projects agency) performed the twitter bot challenge, with the goal of identifying influence-bots that supported pro vaccination discussions on twitter. 6 teams(2 universities and some private companies) competed, with Sentimetrix winning. \n",
    "\n",
    "This is a description of the bot detection approaches used in the competition. It is intended to make for a definition of twitter bots, although it isn’t fully conclusive, as Bots were, and still are, hard to detect, despite research innovations within this field. \n",
    "\n",
    "A big part of the detection of bots, comes from machine learning, but the teams found that machine learning itself wasn’t sufficient. \n",
    "They also had to look at;\n",
    "#### Tweet syntax\n",
    "Does the user post tweets whose syntax is similar to the natural language generation program called Eliza and auto-generation of language.\n",
    "The average number of hashtags, user mentions, links, special characters, retweets, geo-enabled tweets, percentage of tweets ending with punctuation, hashtag or link\n",
    "#### Tweet semantics\n",
    "Number of user posts related to vaccination\n",
    "Users general opinion of the subject in question (in this case anti-vaccination). Are there any contradictions between opinions?  \n",
    "The most frequent topics the user tweets about\n",
    "The number of languages used by the user - if there are several languages being used, the rationale is that it has a higher tendency of being a bot\n",
    "Inconsistency and URL replacements in retweets which directs users to sites that pays the “bot”-makers. \n",
    "\n",
    "#### Temporal behaviour features\n",
    "Did the user linger between opinions over time? \n",
    "Did the user engage users with anti-vaccination opinions, and then switching to a different view? \n",
    "Are there regularities in the tweets, meaning is the tweets tweeted with some specific algorithmic time schedule?\n",
    "The duration of tweeting - is the user tweeting constantly for 10 minuts, for an hour etc. \n",
    "Average number of tweets per day\n",
    "Percentage of dropped followers\n",
    "Is there any abrupt changes in the users’ metadata? (followers, followees, posts). \n",
    "\n",
    "#### User profile features\n",
    "Did the user have a profile picture? From a stock site?\n",
    "Did the users profile have an associated URL? Was the URL meaningful, or just random numbers for example. \n",
    "How is the users twitter name? A standard string, or an integer? \n",
    "Number of posts/retweets/replies/mentions\n",
    "Number of followers/followings\n",
    "Number of sources used by the user such as mobile applications, desktop browsers, ‘null’ or anything else? \n",
    "GPS coordinate availability for user’s tweets\n",
    "Similarity of users to known bots\n",
    "\n",
    "#### Network features (We can make use of Gephi for doing this)\n",
    "Average deviation of user sentiment scores from those following and followees. \n",
    "In and out degree centrality\n",
    "Average clustering coefficient of retweet and mention network associated with each user\n",
    "Pagerank and betweenness centrality of users in both retweet and mention networks\n",
    "Variables related to star and clique networks associated with users. \n",
    "Number of known bots followed by a user - a user following several known bots is more likely to be a bot\n",
    "number/percentage of bots in the cluster that a user belonged to - if a clustering algorithm places the user in a cluster with many bots, he is more likely to be a bot.\n",
    "\n",
    "#### Socialbots and their friends (Robert W. Gehl & Maria Bakardjieva: \n",
    "The first definition that is presented in the book is what they call “a crisp technical definition”. It is made by engineers and it has its starting point in their research which was very innovative when it was done (page 1). It is as follows:\n",
    "“A socialbot is an automation software that controls an account on a par- ticular OSN [Online Social Network], and has the ability to perform basic activities such as posting a message and sending a connection request. What makes a socialbot different from self-declared bots (e.g., Twitter bots that post up-to-date weather forecasts) and spambots is that it is designed to be stealthy, that is, it is able to pass itself off as a human being “ (page 1).\n",
    "In this definition, the authors dissociate a socialbots from e.g. a Twitter bots, which is very relevant for our project. Its relevant in two different ways: at first, it outlines the big difference between socialbots and Twitter bots which is that socialbots are designed to be regarded as human beings. With this fact in mind the other relevant part is that the author of the definition wouldn’t regard Twitter bots that performs simple task as posting up-to-date weather forecast as a socialbot. Retweeting arguments related to a debate wouldn’t be considered more sophisticated than posting weather updates, or would it? For the Twitter bot to be part of a debate the owner of the bot must have some kind of part in the debate. If the owner wants the bot to partake in the debate and to send a certain message it requires some engineering of the bot so that it appears realistic. \n",
    "\n",
    "#### Characteristics of Social bots (definition 1):\n",
    "-designed to be stealthy (page 1)\n",
    "\n",
    "-designed to present a “self” (page 1)\n",
    "\n",
    "-designed to perform undesirable tasks (page 1)\n",
    "\n",
    "The second definition seems more concurrent in relation to our projects and experience. It is as follows:\n",
    " \n",
    "Socialbots are software processes that are programmed to appear to be human-generated within the context of social networking sites (SNSs) such as Facebook and Twitter. They achieve their ‘humanness’ by either mim icking other SNS users or through artificial intelligence that simulates human users of social networking sites. They share pictures, post status updates and Tweets, enter into conversations with other SNS users, and make and accept friend and follower requests. Importantly, they are designed to appear human to both SNS users as well as the SNS platform itself. Their goals are various, but often include shaping the online interactions and social networking practices of users (page 2).\n",
    "This definition differs from the first one in several ways. Taking in account the first line it is clear that this definition highlights the human aspect of these bots. They appear to be human-generated within a certain context.\n",
    " \n",
    "The definition explains the methodical way of making the bots intervene in social networks in ways that makes their appearance trustworthy. It is explained how it is done: “… by either mim icking other SNS users or through artificial intelligence that simulates human users of social networking sites. They share pictures, post status updates and Tweets, enter into conversations with other SNS users, and make and accept friend and follower requests”.\n",
    " \n",
    "The most important attribute to the “modern” Socialbots are their complex goals. In our case it is as mentioned in the definition: “…but often include shaping the online interactions and social networking practices of users”. The clear purpose of the bots is what is both really interesting but also complicated. This may not always appear transparent.\n",
    " \n",
    "#### Characteristics of Social bots (definition 2):\n",
    "-designed to be human-generated within the context of SNS (page 2)\n",
    "\n",
    "-designed to mimick other users of SNS to achieve “humanness” (page 2)\n",
    "\n",
    "-designed to intervene in social networking practices (page 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
